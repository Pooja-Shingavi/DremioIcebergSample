{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4dc2d66-6e17-4501-aa08-c655a68a0351",
   "metadata": {},
   "source": [
    "Setup and spark config- connections to nessie catalog and minio objectstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09185784-4b1d-4636-b9e6-f8531d4a22f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/docker/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/docker/.ivy2/cache\n",
      "The jars for the packages stored in: /home/docker/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0eb70725-f1cb-43d0-a578-61d8d9d3ee1f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.91.3 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.20.131 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.20.131 in central\n",
      "\tfound software.amazon.awssdk#utils;2.20.131 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.20.131 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.20.131 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.20.131 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.2/iceberg-spark-runtime-3.5_2.12-1.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2!iceberg-spark-runtime-3.5_2.12.jar (96260ms)\n",
      "downloading https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.5_2.12/0.91.3/nessie-spark-extensions-3.5_2.12-0.91.3.jar ...\n",
      "\t[SUCCESSFUL ] org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.91.3!nessie-spark-extensions-3.5_2.12.jar (7731ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.20.131/bundle-2.20.131.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#bundle;2.20.131!bundle.jar (1236865ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/url-connection-client/2.20.131/url-connection-client-2.20.131.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#url-connection-client;2.20.131!url-connection-client.jar (465ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/eventstream/eventstream/1.0.1/eventstream-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.eventstream#eventstream;1.0.1!eventstream.jar (316ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/utils/2.20.131/utils-2.20.131.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#utils;2.20.131!utils.jar (740ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/annotations/2.20.131/annotations-2.20.131.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#annotations;2.20.131!annotations.jar (125ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/http-client-spi/2.20.131/http-client-spi-2.20.131.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#http-client-spi;2.20.131!http-client-spi.jar (325ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (87ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (159ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/metrics-spi/2.20.131/metrics-spi-2.20.131.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#metrics-spi;2.20.131!metrics-spi.jar (175ms)\n",
      ":: resolution report :: resolve 4242ms :: artifacts dl 1343342ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.91.3 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0eb70725-f1cb-43d0-a578-61d8d9d3ee1f\n",
      "\tconfs: [default]\n",
      "\t11 artifacts copied, 0 already retrieved (528933kB/311ms)\n",
      "25/01/04 20:34:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          name|\n",
      "+--------------+\n",
      "|Pooja Shingavi|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "NESSIE_SERVER_URI = \"http://172.19.0.3:19120/api/v2\"\n",
    "WAREHOUSE_BUCKET = \"s3://warehouse\"\n",
    "MINIO_URI = \"http://172.19.0.2:9000\"\n",
    "\n",
    "\n",
    "## Configurations for Spark Session\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "  \t\t#packages\n",
    "        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.91.3,software.amazon.awssdk:bundle:2.20.131,software.amazon.awssdk:url-connection-client:2.20.131')\n",
    "  \t\t#SQL Extensions\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "  \t\t#Configuring Catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', NESSIE_SERVER_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set(\"spark.sql.catalog.nessie.s3.endpoint\",MINIO_URI)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', WAREHOUSE_BUCKET)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    ")\n",
    "\n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark Running\")\n",
    "\n",
    "## TEST QUERY TO CHECK IT WORKING\n",
    "### Create TABLE\n",
    "spark.sql(\"CREATE TABLE nessie.example (name STRING) USING iceberg;\").show()\n",
    "### INSERT INTO TABLE\n",
    "spark.sql(\"INSERT INTO nessie.example VALUES ('Pooja Shingavi');\").show()\n",
    "### Query Table\n",
    "spark.sql(\"SELECT * FROM nessie.example;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a48f3-11f3-4a9d-b2cb-aa8b971ed6b7",
   "metadata": {},
   "source": [
    "Create table from spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1416119b-0169-448c-b0ac-979bc1f644da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceberg table created using DataFrames\n",
      "+---+-----+--------------------+\n",
      "| id| name|                  ts|\n",
      "+---+-----+--------------------+\n",
      "|  1|Pooja|2025-01-04 20:36:...|\n",
      "|  2|  Foo|2025-01-04 20:36:...|\n",
      "|  3|  Bar|2025-01-04 20:36:...|\n",
      "+---+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Create DataFrame\n",
    "data = [(1, \"Pooja\"), (2, \"Foo\"), (3, \"Bar\")]\n",
    "columns = [\"id\", \"name\"]\n",
    "df = spark.createDataFrame(data, columns).withColumn(\"ts\", current_timestamp())\n",
    "\n",
    "# Write DataFrame to Iceberg table\n",
    "df.writeTo(\"nessie.df_table\").create()\n",
    "print(\"Iceberg table created using DataFrames\")\n",
    "\n",
    "#query table\n",
    "spark.sql(\"SELECT * FROM nessie.df_table;\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b542a-4217-4939-92a4-3965b397a1e3",
   "metadata": {},
   "source": [
    "Create Table using SQL statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b5a87f-049d-442b-b97d-22fccfc85004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|          name|\n",
      "+---+--------------+\n",
      "|  1|Pooja Shingavi|\n",
      "|  2|       Foo Bar|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an Iceberg table using SQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.sql_table (\n",
    "        id INT,\n",
    "        name STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"INSERT INTO nessie.sql_table VALUES (1, 'Pooja Shingavi'), (2, 'Foo Bar');\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM nessie.sql_table;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b4460a-62ce-4097-980d-8cfbf5a3d8b1",
   "metadata": {},
   "source": [
    "Inserting Data Into an Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a964d7-3d50-41a8-aa97-92f2422d2a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------------+\n",
      "| id| name|                 ts|\n",
      "+---+-----+-------------------+\n",
      "|  1|Pooja|2025-01-04 12:00:00|\n",
      "|  2|  Foo|2025-01-04 12:05:00|\n",
      "|  3|  Bar|2025-01-04 12:10:00|\n",
      "+---+-----+-------------------+\n",
      "\n",
      "Namespace 'db' created in Nessie\n",
      "Iceberg table created with no records\n",
      "Data inserted into Iceberg table from JSON DataFrame\n",
      "+---+-----+-------------------+\n",
      "| id| name|                 ts|\n",
      "+---+-----+-------------------+\n",
      "|  1|Pooja|2025-01-04 12:00:00|\n",
      "|  2|  Foo|2025-01-04 12:05:00|\n",
      "|  3|  Bar|2025-01-04 12:10:00|\n",
      "+---+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 1: Create a JSON file with a few records\n",
    "json_data = [\n",
    "    {\"id\": 1, \"name\": \"Pooja\", \"ts\": \"2025-01-04T12:00:00\"},\n",
    "    {\"id\": 2, \"name\": \"Foo\", \"ts\": \"2025-01-04T12:05:00\"},\n",
    "    {\"id\": 3, \"name\": \"Bar\", \"ts\": \"2025-01-04T12:10:00\"}\n",
    "]\n",
    "\n",
    "json_file_path = \"/tmp/data.json\"\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(json_data, json_file)\n",
    "\n",
    "# Step 2: Read the JSON file into a DataFrame\n",
    "df = spark.read.json(json_file_path)\n",
    "df = df.withColumn(\"ts\", col(\"ts\").cast(\"timestamp\"))\n",
    "df.show()\n",
    "\n",
    "# Step 3: Create the \"db\" namespace in Nessie\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.db\")\n",
    "print(\"Namespace 'db' created in Nessie\")\n",
    "\n",
    "# Step 4: Create an empty Iceberg table using SQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS nessie.db.example_table (\n",
    "        id INT,\n",
    "        name STRING,\n",
    "        ts TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")\n",
    "print(\"Iceberg table created with no records\")\n",
    "\n",
    "# Step 5: Insert the data from JSON DataFrame into the Iceberg table\n",
    "df.writeTo(\"nessie.db.example_table\").append()\n",
    "print(\"Data inserted into Iceberg table from JSON DataFrame\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM nessie.db.example_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6281eb5-4ef7-4e9e-ad01-c7876f5e1b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using MERGE INTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a45f65-31b3-4f55-81e8-81733c2868ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-------------------+\n",
      "|count| id| name|                 ts|\n",
      "+-----+---+-----+-------------------+\n",
      "| NULL|  1|Pooja|2025-01-04 12:00:00|\n",
      "|    5|  2|  Foo|2025-01-04 12:05:00|\n",
      "|   10|  3|  Bar|2025-01-04 12:10:00|\n",
      "+-----+---+-----+-------------------+\n",
      "\n",
      "Namespace 'merge_example' created in Nessie\n",
      "Iceberg table created with no records\n",
      "Initial data inserted into Iceberg table\n",
      "+---+-----+-----+-------------------+\n",
      "| id| name|count|                 ts|\n",
      "+---+-----+-----+-------------------+\n",
      "|  1|Pooja| NULL|2025-01-04 12:00:00|\n",
      "|  2|  Foo|    5|2025-01-04 12:05:00|\n",
      "|  3|  Bar|   10|2025-01-04 12:10:00|\n",
      "+---+-----+-----+-------------------+\n",
      "\n",
      "+-----+---+------+---------+\n",
      "|count| id|  name|       op|\n",
      "+-----+---+------+---------+\n",
      "|    1|  1| Pooja|increment|\n",
      "|    1|  2|   Foo|increment|\n",
      "| NULL|  3|   Bar|   delete|\n",
      "|    1|  4|Funion|   insert|\n",
      "+-----+---+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merged into Iceberg table using MERGE INTO\n",
      "+---+------+-----+--------------------+\n",
      "| id|  name|count|                  ts|\n",
      "+---+------+-----+--------------------+\n",
      "|  1| Pooja|    0| 2025-01-04 12:00:00|\n",
      "|  2|   Foo|    6| 2025-01-04 12:05:00|\n",
      "|  4|Funion|    1|2025-01-04 20:50:...|\n",
      "+---+------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a JSON file with a few records\n",
    "json_data = [\n",
    "    {\"id\": 1, \"name\": \"Pooja\", \"count\": None, \"ts\": \"2025-01-04T12:00:00\"},\n",
    "    {\"id\": 2, \"name\": \"Foo\", \"count\": 5, \"ts\": \"2025-01-04T12:05:00\"},\n",
    "    {\"id\": 3, \"name\": \"Bar\", \"count\": 10, \"ts\": \"2025-01-04T12:10:00\"}\n",
    "]\n",
    "\n",
    "json_file_path = \"/tmp/data.json\"\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(json_data, json_file)\n",
    "\n",
    "# Step 2: Read the JSON file into a DataFrame\n",
    "df = spark.read.json(json_file_path)\n",
    "df = df.withColumn(\"ts\", col(\"ts\").cast(\"timestamp\"))\n",
    "df.createOrReplaceTempView(\"json_table\")\n",
    "df.show()\n",
    "\n",
    "# Step 3: Create the \"db\" namespace in Nessie\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.merge_example\")\n",
    "print(\"Namespace 'merge_example' created in Nessie\")\n",
    "\n",
    "# Step 4: Create an empty Iceberg table using SQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.merge_example.example_table (\n",
    "        id INT,\n",
    "        name STRING,\n",
    "        count INT,\n",
    "        ts TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")\n",
    "print(\"Iceberg table created with no records\")\n",
    "\n",
    "# Insert the initial data into the Iceberg table\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO nessie.merge_example.example_table\n",
    "    SELECT id, name, count, ts FROM json_table\n",
    "\"\"\")\n",
    "print(\"Initial data inserted into Iceberg table\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM nessie.merge_example.example_table;\").show()\n",
    "\n",
    "# Step 5: Create a source DataFrame for updates\n",
    "update_data = [\n",
    "    {\"id\": 1, \"name\": \"Pooja\", \"count\": 1, \"op\": \"increment\"},\n",
    "    {\"id\": 2, \"name\": \"Foo\", \"count\": 1, \"op\": \"increment\"},\n",
    "    {\"id\": 3, \"name\": \"Bar\", \"count\": None, \"op\": \"delete\"},\n",
    "    {\"id\": 4, \"name\": \"Funion\", \"count\": 1, \"op\": \"insert\"}\n",
    "]\n",
    "\n",
    "update_file_path = \"/tmp/update_data.json\"\n",
    "with open(update_file_path, 'w') as update_file:\n",
    "    json.dump(update_data, update_file)\n",
    "\n",
    "source_df = spark.read.json(update_file_path)\n",
    "source_df.createOrReplaceTempView(\"source_table\")\n",
    "source_df.show()\n",
    "\n",
    "# Step 6: Use the MERGE INTO command to update the Iceberg table\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO nessie.merge_example.example_table t\n",
    "    USING source_table s\n",
    "    ON t.id = s.id\n",
    "    WHEN MATCHED AND s.op = 'delete' THEN DELETE\n",
    "    WHEN MATCHED AND t.count IS NULL AND s.op = 'increment' THEN UPDATE SET t.count = 0\n",
    "    WHEN MATCHED AND s.op = 'increment' THEN UPDATE SET t.count = t.count + 1\n",
    "    WHEN NOT MATCHED THEN INSERT (id, name, count, ts) VALUES (s.id, s.name, s.count, current_timestamp())\n",
    "\"\"\")\n",
    "print(\"Data merged into Iceberg table using MERGE INTO\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM nessie.merge_example.example_table;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d7058-7447-4af2-9193-e32a9b5e0dd8",
   "metadata": {},
   "source": [
    "Partitioning with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a171bdb-6f6f-43a2-8a04-d55513697126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the \"partitioning_example\" namespace in Nessie\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.partitioning_example\")\n",
    "print(\"Namespace 'partitioning_example' created in Nessie\")\n",
    "\n",
    "# Step 2: Create an Iceberg table with initial partitioning on the 'name' column\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.partitioning_example.example_table (\n",
    "        id INT,\n",
    "        name STRING,\n",
    "        count INT,\n",
    "        ts TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (name)\n",
    "\"\"\")\n",
    "print(\"Iceberg table created with initial partitioning on 'name'\")\n",
    "\n",
    "# Step 3: Insert initial data into the Iceberg table\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO nessie.partitioning_example.example_table VALUES\n",
    "    (1, 'Pooja', NULL, TIMESTAMP('2024-07-02T12:00:00')),\n",
    "    (2, 'Foo', 5, TIMESTAMP('2024-07-02T12:05:00')),\n",
    "    (3, 'Bar', 10, TIMESTAMP('2024-07-02T12:10:00'))\n",
    "\"\"\")\n",
    "print(\"Initial data inserted into Iceberg table\")\n",
    "\n",
    "# Step 4: Update the partitioning of the Iceberg table to include the 'ts' column\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE nessie.partitioning_example.example_table\n",
    "    ADD PARTITION FIELD bucket(4, ts) AS ts_bucket\n",
    "\"\"\")\n",
    "print(\"Iceberg table partitioning updated to include 'ts' column\")\n",
    "\n",
    "# Step 5: Insert additional data into the Iceberg table\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO nessie.partitioning_example.example_table VALUES\n",
    "    (4, 'Funion', 15, TIMESTAMP('2024-07-02T12:15:00')),\n",
    "    (5, 'Minion', 20, TIMESTAMP('2024-07-02T12:20:00'))\n",
    "\"\"\")\n",
    "print(\"Additional data inserted into Iceberg table\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM nessie.partitioning_example.example_table;\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
